{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cbKMtUOdjPV","outputId":"f98393b4-2695-4f8b-cf86-a0af0ee91812","executionInfo":{"status":"ok","timestamp":1752360755622,"user_tz":420,"elapsed":10905,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import kagglehub\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBta4F_KtE9i","outputId":"9a09f95a-6a2d-47f3-d2c8-bcafd962ae5b","executionInfo":{"status":"ok","timestamp":1752360777614,"user_tz":420,"elapsed":21972,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Final Dataset source from: https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset?select=phishing_email.csv\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = pd.read_csv('/content/drive/MyDrive/ECE592B/archive/phishing_email.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tmLt88QEtITu","executionInfo":{"status":"ok","timestamp":1752360920237,"user_tz":420,"elapsed":142621,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[],"source":["# Data processing\n","df['text'] = df['text_combined']\n","df = df.drop(columns=['text_combined'])\n","df['text'] = df['text'].str.lower()\n","\n","# Clean URLs special characters\n","def clean_text(text):\n","    text = re.sub(r'\\S+@\\S+', ' ', text)  # remove email addresses\n","    text = re.sub(r'http\\S+|www\\S+', ' ', text)  # remove URLs\n","    text = re.sub(r'\\d+', ' ', text)  # remove numbers\n","    text = re.sub(r'[^\\w\\s]', ' ', text)  # remove punctuation\n","    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n","    text = re.sub(r'subject', '', text).strip()# remove subject\n","    return text\n","\n","df['text'] = df['text'].apply(clean_text)\n","\n","# Stop words + Lemmatization = Tokenize\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def tokenize_and_lemmatize(text):\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","df['text'] = df['text'].apply(tokenize_and_lemmatize)\n","\n","df['label_num'] = df['label']\n","df.drop(columns=['label'], inplace=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wPSvmaiVzkY4","executionInfo":{"status":"ok","timestamp":1752360920247,"user_tz":420,"elapsed":5,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6zkC-1UktO5n","executionInfo":{"status":"ok","timestamp":1752360920262,"user_tz":420,"elapsed":18,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[],"source":["X = df['text']\n","y = df['label_num']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.1,\n","    random_state=42,\n","    stratify=y  # Keeps class distribution balanced\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YXhQZQFRtRCa","executionInfo":{"status":"ok","timestamp":1752360920275,"user_tz":420,"elapsed":3,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[],"source":["def pruned_vocab(input_data, percentile):\n","  # Build vocabulary set\n","  V = set()\n","  idf = {}\n","  tf = {}\n","\n","  for i in range(len(input_data)):\n","    terms = input_data.iloc[i].split()\n","    V.update(terms)\n","    tf[i] = {}\n","    for term in terms:\n","        if term not in idf:\n","            idf[term] = 0\n","        idf[term] += 1\n","\n","        if term not in tf[i]:\n","            tf[i][term] = 0\n","        tf[i][term] += 1\n","\n","  # Generate IDF for each term by dividing by total number of documents\n","  N = len(input_data)\n","  for term in idf:\n","      idf[term] = np.log(N / idf[term])\n","\n","  # Find TF-IDF threshold (Xth percentile)\n","  #print(len(input_data))\n","  tf_idfs = []\n","  for i in range(len(input_data)):\n","      for term in tf[i]:\n","          tf_idfs.append(tf[i][term] * idf[term])\n","\n","  threshold = np.percentile(tf_idfs, percentile)\n","\n","  # prune by tf-idf\n","  V_pruned = set()\n","  for i in range(len(input_data)):\n","      for term in tf[i]:\n","          tf_idf = tf[i][term] * idf[term]\n","          if tf_idf >= threshold:\n","              V_pruned.add(term)\n","\n","  return V_pruned"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xFYsryA40GT-","executionInfo":{"status":"ok","timestamp":1752360939249,"user_tz":420,"elapsed":18976,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}}},"outputs":[],"source":["# Get SPAM pruned Vocab\n","train_spam = X_train[y_train == 1]\n","V_spam = pruned_vocab(train_spam, 99)\n","\n","# Get Ham pruned Vocab\n","train_ham = X_train[y_train == 0]\n","V_ham = pruned_vocab(train_ham, 99)\n","\n","# Combine Spam and Ham Vocab\n","V = V_spam.union(V_ham)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c009f5b3","executionInfo":{"status":"ok","timestamp":1752362204665,"user_tz":420,"elapsed":921239,"user":{"displayName":"Shiyi Zhang","userId":"08452486062058068739"}},"outputId":"600bc92f-7f8c-48b4-a2aa-f5fd6d4cb617"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1 Accuracy: 0.9824\n","Fold 2 Accuracy: 0.9810\n","Fold 3 Accuracy: 0.9832\n","Fold 4 Accuracy: 0.9832\n","Fold 5 Accuracy: 0.9817\n","\n","Average Cross-Validation Accuracy: 0.9823\n","\n","Final Test Accuracy: 0.9854527821554128\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.98      3960\n","           1       0.99      0.98      0.99      4289\n","\n","    accuracy                           0.99      8249\n","   macro avg       0.99      0.99      0.99      8249\n","weighted avg       0.99      0.99      0.99      8249\n","\n"]}],"source":["\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# 1. Split 10% final test set\n","X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n","\n","# 2. TF-IDF Vectorizer on train_val data\n","vectorizer = TfidfVectorizer(max_features=5000)\n","X_train_val_tfidf = vectorizer.fit_transform(X_train_val)\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# 3. K-Fold Cross Validation on training set (5 folds)\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","fold_accuracies = []\n","\n","for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val_tfidf)):\n","    X_train, X_val = X_train_val_tfidf[train_idx], X_train_val_tfidf[val_idx]\n","    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n","\n","    clf = RandomForestClassifier(random_state=42)\n","    clf.fit(X_train, y_train)\n","    y_val_pred = clf.predict(X_val)\n","    acc = accuracy_score(y_val, y_val_pred)\n","    fold_accuracies.append(acc)\n","    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n","\n","print(f\"\\nAverage Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f}\")\n","\n","# 4. Train on full training set and test on final 10%\n","final_model = RandomForestClassifier(random_state=42)\n","final_model.fit(X_train_val_tfidf, y_train_val)\n","y_test_pred = final_model.predict(X_test_tfidf)\n","\n","print(\"\\nFinal Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}